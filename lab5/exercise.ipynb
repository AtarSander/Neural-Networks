{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9e1065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import *\n",
    "from model import *\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ff92e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2939"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data/train.pkl\", \"rb\") as f:\n",
    "    train_data = pickle.load(f)\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6478c43d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "histogram = histogram(train_data)\n",
    "plt.bar(histogram.keys(), histogram.values())\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram of Labels in Training Data\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cf6f898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset length</th>\n",
       "      <th>Min sequence length</th>\n",
       "      <th>Max sequence length</th>\n",
       "      <th>Mean sequence length</th>\n",
       "      <th>Unique lengths:</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2939</td>\n",
       "      <td>4</td>\n",
       "      <td>6308</td>\n",
       "      <td>861.128205</td>\n",
       "      <td>819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dataset length  Min sequence length  Max sequence length  \\\n",
       "0            2939                    4                 6308   \n",
       "\n",
       "   Mean sequence length  Unique lengths:  \n",
       "0            861.128205              819  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = len_histogram(train_data)\n",
    "df = pd.DataFrame([{\n",
    "    \"Dataset length\": len(train_data),\n",
    "    \"Min sequence length\": min(lengths.keys()),\n",
    "    \"Max sequence length\": max(lengths.keys()),\n",
    "    \"Mean sequence length\": np.mean(list(lengths.keys())),\n",
    "    \"Unique lengths:\": len(list(lengths.keys()))\n",
    "}])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "848c4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b5e5c7",
   "metadata": {},
   "source": [
    "We split the dataset into 85% training set and 15% validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "155d137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataset = SequenceDataset(train_data)\n",
    "val_dataset = SequenceDataset(val_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "751da5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfb406d",
   "metadata": {},
   "source": [
    "## Training\n",
    "We trained a total of eight sequence classification models, evenly split between two recurrent architectures:\n",
    "- 4 LSTM-based classifiers\n",
    "- 4 GRU-based classifiers\n",
    "\n",
    "All models use a hidden size of 128. Training was performed with the following settings:\n",
    "- Batch size: 64\n",
    "- Learning rate: 3e-4\n",
    "- Maximum epochs: 100\n",
    "- Early stopping: Triggered if validation accuracy does not improve for 10 consecutive epochs (patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d123cf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    LSTMClassifier(input_size=1, hidden_size=128, output_size=len(histogram), num_layers=1),\n",
    "    LSTMClassifier(input_size=1, hidden_size=128, output_size=len(histogram), num_layers=2),\n",
    "    LSTMClassifier(input_size=1, hidden_size=128, output_size=len(histogram), num_layers=1, bidirectional=True),\n",
    "    LSTMClassifier(input_size=1, hidden_size=128, output_size=len(histogram), num_layers=2, bidirectional=True),\n",
    "    GRUClassifier(input_size=1, hidden_size=128, output_size=len(histogram), num_layers=1),\n",
    "    GRUClassifier(input_size=1, hidden_size=128, output_size=len(histogram), num_layers=2),\n",
    "    GRUClassifier(input_size=1, hidden_size=128, output_size=len(histogram), num_layers=1, bidirectional=True),\n",
    "    GRUClassifier(input_size=1, hidden_size=128, output_size=len(histogram), num_layers=2, bidirectional=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5930d8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ Training model 0 ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atarsander/University/Neural-Networks/lab5/utils.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequences = [torch.tensor(seq).unsqueeze(-1).float() for seq in sequences]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss = 1.1745 | Train accuracy = 55.36% | Val accuracy = 56.01%\n",
      "Epoch 10: Loss = 1.0502 | Train accuracy = 61.77% | Val accuracy = 65.31%\n",
      "Epoch 15: Loss = 1.0045 | Train accuracy = 62.29% | Val accuracy = 65.99%\n",
      "Epoch 20: Loss = 0.9346 | Train accuracy = 64.37% | Val accuracy = 67.12%\n",
      "Epoch 25: Loss = 0.8844 | Train accuracy = 67.33% | Val accuracy = 68.71%\n",
      "Epoch 30: Loss = 0.8167 | Train accuracy = 70.30% | Val accuracy = 71.88%\n",
      "Epoch 35: Loss = 0.8150 | Train accuracy = 69.22% | Val accuracy = 71.88%\n",
      "Epoch 40: Loss = 0.7640 | Train accuracy = 72.06% | Val accuracy = 72.56%\n",
      "Epoch 45: Loss = 0.7935 | Train accuracy = 71.38% | Val accuracy = 72.79%\n",
      "Epoch 50: Loss = 0.6999 | Train accuracy = 73.86% | Val accuracy = 72.79%\n",
      "Epoch 55: Loss = 0.6616 | Train accuracy = 75.78% | Val accuracy = 72.34%\n",
      "Epoch 60: Loss = 0.6628 | Train accuracy = 76.42% | Val accuracy = 74.15%\n",
      "Epoch 65: Loss = 0.6033 | Train accuracy = 78.98% | Val accuracy = 73.02%\n",
      "Early stopping triggered\n",
      "------------------------------ Training model 1 ------------------------------\n",
      "Epoch 5: Loss = 1.0622 | Train accuracy = 60.89% | Val accuracy = 63.49%\n",
      "Epoch 10: Loss = 1.0320 | Train accuracy = 62.37% | Val accuracy = 67.35%\n",
      "Epoch 15: Loss = 0.8954 | Train accuracy = 66.41% | Val accuracy = 70.07%\n",
      "Epoch 20: Loss = 0.8825 | Train accuracy = 67.57% | Val accuracy = 69.39%\n",
      "Epoch 25: Loss = 0.8438 | Train accuracy = 70.14% | Val accuracy = 70.98%\n",
      "Epoch 30: Loss = 0.7423 | Train accuracy = 71.78% | Val accuracy = 72.79%\n",
      "Epoch 35: Loss = 0.7158 | Train accuracy = 73.70% | Val accuracy = 73.24%\n",
      "Epoch 40: Loss = 0.6966 | Train accuracy = 74.54% | Val accuracy = 73.24%\n",
      "Epoch 45: Loss = 0.7300 | Train accuracy = 73.26% | Val accuracy = 66.21%\n",
      "Early stopping triggered\n",
      "------------------------------ Training model 2 ------------------------------\n",
      "Epoch 5: Loss = 1.0606 | Train accuracy = 60.25% | Val accuracy = 64.40%\n",
      "Epoch 10: Loss = 0.9240 | Train accuracy = 64.33% | Val accuracy = 65.76%\n",
      "Epoch 15: Loss = 0.8446 | Train accuracy = 67.69% | Val accuracy = 67.12%\n",
      "Epoch 20: Loss = 0.7550 | Train accuracy = 71.02% | Val accuracy = 71.66%\n",
      "Epoch 25: Loss = 0.8050 | Train accuracy = 70.86% | Val accuracy = 73.02%\n",
      "Epoch 30: Loss = 0.6748 | Train accuracy = 75.02% | Val accuracy = 74.15%\n",
      "Epoch 35: Loss = 0.6475 | Train accuracy = 76.26% | Val accuracy = 72.56%\n",
      "Epoch 40: Loss = 0.6202 | Train accuracy = 77.42% | Val accuracy = 73.70%\n",
      "Epoch 45: Loss = 0.5824 | Train accuracy = 79.62% | Val accuracy = 76.87%\n",
      "Epoch 50: Loss = 0.5489 | Train accuracy = 81.31% | Val accuracy = 75.28%\n",
      "Epoch 55: Loss = 0.4739 | Train accuracy = 83.15% | Val accuracy = 75.28%\n",
      "Early stopping triggered\n",
      "------------------------------ Training model 3 ------------------------------\n",
      "Epoch 5: Loss = 0.9827 | Train accuracy = 61.61% | Val accuracy = 63.27%\n",
      "Epoch 10: Loss = 0.9322 | Train accuracy = 64.57% | Val accuracy = 63.49%\n",
      "Epoch 15: Loss = 0.8726 | Train accuracy = 66.37% | Val accuracy = 65.53%\n",
      "Epoch 20: Loss = 0.7671 | Train accuracy = 71.30% | Val accuracy = 66.67%\n",
      "Epoch 25: Loss = 0.6955 | Train accuracy = 73.74% | Val accuracy = 72.34%\n",
      "Epoch 30: Loss = 0.6755 | Train accuracy = 75.62% | Val accuracy = 70.98%\n",
      "Epoch 35: Loss = 0.5906 | Train accuracy = 78.94% | Val accuracy = 72.11%\n",
      "Epoch 40: Loss = 0.6266 | Train accuracy = 77.10% | Val accuracy = 72.34%\n",
      "Epoch 45: Loss = 0.5406 | Train accuracy = 80.58% | Val accuracy = 73.02%\n",
      "Epoch 50: Loss = 0.4699 | Train accuracy = 84.03% | Val accuracy = 74.15%\n",
      "Epoch 55: Loss = 0.3936 | Train accuracy = 86.19% | Val accuracy = 75.51%\n",
      "Epoch 60: Loss = 0.3345 | Train accuracy = 88.27% | Val accuracy = 76.87%\n",
      "Epoch 65: Loss = 0.4953 | Train accuracy = 81.59% | Val accuracy = 75.06%\n",
      "Epoch 70: Loss = 0.2586 | Train accuracy = 91.95% | Val accuracy = 76.42%\n",
      "Early stopping triggered\n",
      "------------------------------ Training model 4 ------------------------------\n",
      "Epoch 5: Loss = 1.1887 | Train accuracy = 56.57% | Val accuracy = 58.96%\n",
      "Epoch 10: Loss = 1.0718 | Train accuracy = 59.21% | Val accuracy = 61.68%\n",
      "Epoch 15: Loss = 0.9710 | Train accuracy = 64.17% | Val accuracy = 65.53%\n",
      "Epoch 20: Loss = 0.8506 | Train accuracy = 67.89% | Val accuracy = 69.16%\n",
      "Epoch 25: Loss = 0.8735 | Train accuracy = 69.54% | Val accuracy = 70.98%\n",
      "Epoch 30: Loss = 0.7571 | Train accuracy = 73.26% | Val accuracy = 65.99%\n",
      "Epoch 35: Loss = 0.7112 | Train accuracy = 75.14% | Val accuracy = 73.47%\n",
      "Epoch 40: Loss = 0.6563 | Train accuracy = 76.46% | Val accuracy = 75.51%\n",
      "Epoch 45: Loss = 0.6017 | Train accuracy = 77.74% | Val accuracy = 77.10%\n",
      "Epoch 50: Loss = 0.5740 | Train accuracy = 78.86% | Val accuracy = 77.55%\n",
      "Epoch 55: Loss = 0.5526 | Train accuracy = 79.78% | Val accuracy = 76.19%\n",
      "Epoch 60: Loss = 0.5421 | Train accuracy = 80.22% | Val accuracy = 77.32%\n",
      "Epoch 65: Loss = 0.4961 | Train accuracy = 81.27% | Val accuracy = 79.37%\n",
      "Epoch 70: Loss = 0.4801 | Train accuracy = 82.07% | Val accuracy = 79.82%\n",
      "Epoch 75: Loss = 0.4923 | Train accuracy = 82.47% | Val accuracy = 79.37%\n",
      "Epoch 80: Loss = 0.4511 | Train accuracy = 83.47% | Val accuracy = 79.14%\n",
      "Early stopping triggered\n",
      "------------------------------ Training model 5 ------------------------------\n",
      "Epoch 5: Loss = 1.1543 | Train accuracy = 57.73% | Val accuracy = 60.54%\n",
      "Epoch 10: Loss = 0.9513 | Train accuracy = 63.25% | Val accuracy = 66.89%\n",
      "Epoch 15: Loss = 0.8592 | Train accuracy = 68.17% | Val accuracy = 67.80%\n",
      "Epoch 20: Loss = 0.7578 | Train accuracy = 73.30% | Val accuracy = 73.47%\n",
      "Epoch 25: Loss = 0.6346 | Train accuracy = 75.74% | Val accuracy = 76.42%\n",
      "Epoch 30: Loss = 0.6620 | Train accuracy = 76.30% | Val accuracy = 78.00%\n",
      "Epoch 35: Loss = 0.5266 | Train accuracy = 80.90% | Val accuracy = 77.10%\n",
      "Epoch 40: Loss = 0.4921 | Train accuracy = 81.99% | Val accuracy = 78.46%\n",
      "Epoch 45: Loss = 0.5545 | Train accuracy = 78.78% | Val accuracy = 79.59%\n",
      "Epoch 50: Loss = 0.4175 | Train accuracy = 84.63% | Val accuracy = 78.91%\n",
      "Epoch 55: Loss = 0.3677 | Train accuracy = 87.03% | Val accuracy = 76.19%\n",
      "Epoch 60: Loss = 0.3571 | Train accuracy = 87.35% | Val accuracy = 81.41%\n",
      "Epoch 65: Loss = 0.3037 | Train accuracy = 89.83% | Val accuracy = 80.50%\n",
      "Epoch 70: Loss = 0.3159 | Train accuracy = 89.75% | Val accuracy = 80.95%\n",
      "Epoch 75: Loss = 0.2526 | Train accuracy = 91.51% | Val accuracy = 81.63%\n",
      "Epoch 80: Loss = 0.2246 | Train accuracy = 92.79% | Val accuracy = 81.41%\n",
      "Early stopping triggered\n",
      "------------------------------ Training model 6 ------------------------------\n",
      "Epoch 5: Loss = 1.0820 | Train accuracy = 59.97% | Val accuracy = 60.32%\n",
      "Epoch 10: Loss = 0.9824 | Train accuracy = 62.69% | Val accuracy = 63.95%\n",
      "Epoch 15: Loss = 0.8840 | Train accuracy = 66.69% | Val accuracy = 64.85%\n",
      "Epoch 20: Loss = 0.8089 | Train accuracy = 69.98% | Val accuracy = 70.75%\n",
      "Epoch 25: Loss = 0.7209 | Train accuracy = 73.42% | Val accuracy = 73.70%\n",
      "Epoch 30: Loss = 0.6941 | Train accuracy = 74.82% | Val accuracy = 73.02%\n",
      "Epoch 35: Loss = 0.6329 | Train accuracy = 77.66% | Val accuracy = 76.42%\n",
      "Epoch 40: Loss = 0.5709 | Train accuracy = 79.10% | Val accuracy = 76.19%\n",
      "Epoch 45: Loss = 0.5672 | Train accuracy = 79.30% | Val accuracy = 76.64%\n",
      "Epoch 50: Loss = 0.5212 | Train accuracy = 81.51% | Val accuracy = 78.91%\n",
      "Epoch 55: Loss = 0.4869 | Train accuracy = 82.31% | Val accuracy = 79.37%\n",
      "Epoch 60: Loss = 0.4719 | Train accuracy = 83.83% | Val accuracy = 78.23%\n",
      "Early stopping triggered\n",
      "------------------------------ Training model 7 ------------------------------\n",
      "Epoch 5: Loss = 0.9730 | Train accuracy = 62.61% | Val accuracy = 64.85%\n",
      "Epoch 10: Loss = 0.8447 | Train accuracy = 67.09% | Val accuracy = 67.80%\n",
      "Epoch 15: Loss = 0.7246 | Train accuracy = 73.78% | Val accuracy = 73.92%\n",
      "Epoch 20: Loss = 0.6052 | Train accuracy = 77.50% | Val accuracy = 76.19%\n",
      "Epoch 25: Loss = 0.5386 | Train accuracy = 81.47% | Val accuracy = 77.10%\n",
      "Epoch 30: Loss = 0.4504 | Train accuracy = 83.71% | Val accuracy = 78.91%\n",
      "Epoch 35: Loss = 0.4081 | Train accuracy = 85.63% | Val accuracy = 78.91%\n",
      "Epoch 40: Loss = 0.3685 | Train accuracy = 87.07% | Val accuracy = 79.82%\n",
      "Epoch 45: Loss = 0.4265 | Train accuracy = 83.99% | Val accuracy = 79.37%\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "results = measure_performance(models, train_loader, val_loader, device, epochs=100, lr=3e-4, patience=10, path=\"models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c563479f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training accuracy</th>\n",
       "      <th>Validation accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTMClassifier(\\n  (lstm): LSTM(1, 128, batch_...</td>\n",
       "      <td>79.7%</td>\n",
       "      <td>73.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LSTMClassifier(\\n  (lstm): LSTM(1, 128, num_la...</td>\n",
       "      <td>74.66%</td>\n",
       "      <td>72.56%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTMClassifier(\\n  (lstm): LSTM(1, 128, batch_...</td>\n",
       "      <td>83.67%</td>\n",
       "      <td>75.28%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LSTMClassifier(\\n  (lstm): LSTM(1, 128, num_la...</td>\n",
       "      <td>94.2%</td>\n",
       "      <td>77.55%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GRUClassifier(\\n  (gru): GRU(1, 128, batch_fir...</td>\n",
       "      <td>83.55%</td>\n",
       "      <td>79.14%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GRUClassifier(\\n  (gru): GRU(1, 128, num_layer...</td>\n",
       "      <td>92.83%</td>\n",
       "      <td>81.41%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GRUClassifier(\\n  (gru): GRU(1, 128, batch_fir...</td>\n",
       "      <td>85.55%</td>\n",
       "      <td>78.68%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GRUClassifier(\\n  (gru): GRU(1, 128, num_layer...</td>\n",
       "      <td>88.39%</td>\n",
       "      <td>78.23%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model Training accuracy  \\\n",
       "0  LSTMClassifier(\\n  (lstm): LSTM(1, 128, batch_...             79.7%   \n",
       "1  LSTMClassifier(\\n  (lstm): LSTM(1, 128, num_la...            74.66%   \n",
       "2  LSTMClassifier(\\n  (lstm): LSTM(1, 128, batch_...            83.67%   \n",
       "3  LSTMClassifier(\\n  (lstm): LSTM(1, 128, num_la...             94.2%   \n",
       "4  GRUClassifier(\\n  (gru): GRU(1, 128, batch_fir...            83.55%   \n",
       "5  GRUClassifier(\\n  (gru): GRU(1, 128, num_layer...            92.83%   \n",
       "6  GRUClassifier(\\n  (gru): GRU(1, 128, batch_fir...            85.55%   \n",
       "7  GRUClassifier(\\n  (gru): GRU(1, 128, num_layer...            88.39%   \n",
       "\n",
       "  Validation accuracy  \n",
       "0               73.7%  \n",
       "1              72.56%  \n",
       "2              75.28%  \n",
       "3              77.55%  \n",
       "4              79.14%  \n",
       "5              81.41%  \n",
       "6              78.68%  \n",
       "7              78.23%  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(results, columns=[\"Model\", \"Training accuracy\", \"Validation accuracy\"])\n",
    "results [\"Training accuracy\"] = results [\"Training accuracy\"].apply(lambda x: f\"{round(float(x.strip('%')), 2)}%\")\n",
    "results [\"Validation accuracy\"] = results [\"Validation accuracy\"].apply(lambda x: f\"{round(float(x.strip('%')), 2)}%\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fb8459",
   "metadata": {},
   "source": [
    "The best performance (validation accuracy) was achieved by GRU Classifier with 2 layers (model 5). The worst performance was achieved by LSTM Classifier with 1 layer.\n",
    "### Further experiments\n",
    "We took the GRU Classifier with 2 layers as our final architecture type and conducted more experiments with different learning rates and hidden sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda4395",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_models = [\n",
    "    GRUClassifier(\n",
    "        input_size=1, hidden_size=128, output_size=len(histogram), num_layers=2\n",
    "    ),\n",
    "    GRUClassifier(\n",
    "        input_size=1, hidden_size=64, output_size=len(histogram), num_layers=2\n",
    "    ),\n",
    "    GRUClassifier(\n",
    "        input_size=1, hidden_size=64, output_size=len(histogram), num_layers=2\n",
    "    ),\n",
    "]\n",
    "learning_rates = [3e-3, 3e-3, 3e-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f0e59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ Training model 0 ------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atarsander/University/Neural-Networks/lab5/utils.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequences = [torch.tensor(seq).unsqueeze(-1).float() for seq in sequences]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss = 0.0249 | Train accuracy = 99.40% | Val accuracy = 80.27%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results_final = \u001b[43mtest_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_models\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rates\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mtest_models\u001b[39m\u001b[34m(models, train_loader, val_loader, device, lrs, epochs, patience, path)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(models):\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m30\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m30\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlrs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfinal_model_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     train_preds, train_labels = predict(model, train_loader, device)\n\u001b[32m     16\u001b[39m     train_accuracy = evaluate(train_preds, train_labels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Neural-Networks/lab5/model.py:115\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, dataloader, val_dataloader, device, epochs, lr, patience, file_path)\u001b[39m\n\u001b[32m    112\u001b[39m x, lengths, y = x.to(device).float(), lengths.to(device), y.to(device)\n\u001b[32m    114\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m loss = criterion(output, y)\n\u001b[32m    118\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/anaconda3/envs/ssne/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/anaconda3/envs/ssne/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/University/Neural-Networks/lab5/model.py:59\u001b[39m, in \u001b[36mGRUClassifier.forward\u001b[39m\u001b[34m(self, x, lengths)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, lengths):\n\u001b[32m     56\u001b[39m     packed_input = pack_padded_sequence(\n\u001b[32m     57\u001b[39m         x, lengths.cpu(), batch_first=\u001b[38;5;28;01mTrue\u001b[39;00m, enforce_sorted=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     58\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     packed_output, hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bidirectional:\n\u001b[32m     61\u001b[39m         hidden_cat = torch.cat((hidden[-\u001b[32m2\u001b[39m], hidden[-\u001b[32m1\u001b[39m]), dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/anaconda3/envs/ssne/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/anaconda3/envs/ssne/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programs/anaconda3/envs/ssne/lib/python3.13/site-packages/torch/nn/modules/rnn.py:1405\u001b[39m, in \u001b[36mGRU.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1393\u001b[39m     result = _VF.gru(\n\u001b[32m   1394\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1395\u001b[39m         hx,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1402\u001b[39m         \u001b[38;5;28mself\u001b[39m.batch_first,\n\u001b[32m   1403\u001b[39m     )\n\u001b[32m   1404\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1405\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1410\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1411\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1412\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1413\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1414\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1415\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1416\u001b[39m output = result[\u001b[32m0\u001b[39m]\n\u001b[32m   1417\u001b[39m hidden = result[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# results_final = test_models(final_models, train_loader, val_loader, device, learning_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "87d071cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atarsander/University/Neural-Networks/lab5/utils.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequences = [torch.tensor(seq).unsqueeze(-1).float() for seq in sequences]\n"
     ]
    }
   ],
   "source": [
    "results_final = []\n",
    "for i, model in enumerate(final_models):\n",
    "    model = torch.load(f\"models/final_model_{i}.pth\", map_location=device, weights_only=False)\n",
    "    train_preds, train_labels = predict(model, train_loader, device)\n",
    "    train_accuracy = evaluate(train_preds, train_labels)\n",
    "    val_preds, val_labels = predict(model, val_loader, device)\n",
    "    val_accuracy = evaluate(val_preds, val_labels)\n",
    "    results_final.append((str(model), train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c79cc01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training accuracy</th>\n",
       "      <th>Validation accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GRUClassifier(\\n  (gru): GRU(1, 128, num_layer...</td>\n",
       "      <td>tensor(0.9896)</td>\n",
       "      <td>tensor(0.8027)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GRUClassifier(\\n  (gru): GRU(1, 64, num_layers...</td>\n",
       "      <td>tensor(0.9339)</td>\n",
       "      <td>tensor(0.8163)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GRUClassifier(\\n  (gru): GRU(1, 64, num_layers...</td>\n",
       "      <td>tensor(0.8483)</td>\n",
       "      <td>tensor(0.8005)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model Training accuracy  \\\n",
       "0  GRUClassifier(\\n  (gru): GRU(1, 128, num_layer...    tensor(0.9896)   \n",
       "1  GRUClassifier(\\n  (gru): GRU(1, 64, num_layers...    tensor(0.9339)   \n",
       "2  GRUClassifier(\\n  (gru): GRU(1, 64, num_layers...    tensor(0.8483)   \n",
       "\n",
       "  Validation accuracy  \n",
       "0      tensor(0.8027)  \n",
       "1      tensor(0.8163)  \n",
       "2      tensor(0.8005)  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final = pd.DataFrame(results_final, columns=[\"Model\", \"Training accuracy\", \"Validation accuracy\"])\n",
    "# df_final[\"Training accuracy\"] = df_final[\"Training accuracy\"].apply(\n",
    "#     lambda x: f\"{x.item():.2f}%\" if torch.is_tensor(x) else f\"{float(x):.2f}%\"\n",
    "# )\n",
    "\n",
    "# df_final[\"Validation accuracy\"] = df_final[\"Validation accuracy\"].apply(\n",
    "#     lambda x: f\"{x.item():.2f}%\" if torch.is_tensor(x) else f\"{float(x):.2f}%\"\n",
    "# )\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b617f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = final_models[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9fa3c073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1103\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/test_no_target.pkl\", \"rb\") as f:\n",
    "    test_data = pickle.load(f)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4f172807",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestDataset(test_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "af8149e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_105811/768936045.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequences = [torch.tensor(seq).unsqueeze(-1).float() for seq in sequences]\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_final(final_model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7c7e6872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1103\n"
     ]
    }
   ],
   "source": [
    "pred = pd.DataFrame(predictions)\n",
    "print(len(pred))\n",
    "pred.to_csv(\"pred.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee8763d",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "- The best performance in the initial experiment was achieved by GRU Classifier with 2 layers, with validation accuracy of 81.41%.\n",
    "- In the following experiment the result was slightly improved for the same classifier with smaller hidden size (64).\n",
    "- All of the GRU model's showed better generalization, train/val stability and lower overfitting risk than LSTM's.\n",
    "- Bidirectional LSTM with 2 layers (model 3) showed significant overfitting with 94.2% traning accuracy and 77.55% validation accuracy.\n",
    "- During every model's training it was necessary to use early stopping before target epochs=100, because of falling validation accuracy (patience=10)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
