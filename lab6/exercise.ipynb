{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4c06212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atarsander/Programs/anaconda3/envs/ssne/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from peft import PeftModel\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    CLIPModel,\n",
    "    CLIPProcessor,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    EarlyStoppingCallback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b1a75ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dla mnie faworytem do tytułu będzie Cracovia. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@anonymized_account @anonymized_account Brawo ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@anonymized_account @anonymized_account Super,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@anonymized_account @anonymized_account Musi. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Odrzut natychmiastowy, kwaśna mina, mam problem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  Dla mnie faworytem do tytułu będzie Cracovia. ...      0\n",
       "1  @anonymized_account @anonymized_account Brawo ...      0\n",
       "2  @anonymized_account @anonymized_account Super,...      0\n",
       "3  @anonymized_account @anonymized_account Musi. ...      0\n",
       "4    Odrzut natychmiastowy, kwaśna mina, mam problem      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"data/hate_train.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb59e4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_counts = train_data['label'].value_counts()\n",
    "plt.bar(label_counts.index, label_counts.values)\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Labels in Training Data')\n",
    "plt.xticks([0, 1]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2245fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length: 214\n"
     ]
    }
   ],
   "source": [
    "max_len = train_data[\"sentence\"].apply(len).max()\n",
    "print(f\"Maximum sentence length: {max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53d90490",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def set_tokenizer(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62941d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "967c6f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_roberta = \"xlm-roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba24407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_data[\"sentence\"], train_data[\"label\"], test_size=0.15, random_state=42, stratify=train_data[\"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02af2843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5463, 5.8995], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(train_data[\"label\"]), y=train_data[\"label\"])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4986fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomTextClassificationDataset(\n",
    "    texts=train_texts.tolist(),\n",
    "    labels=train_labels.tolist(),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "val_dataset = CustomTextClassificationDataset(\n",
    "    texts=val_texts.tolist(),\n",
    "    labels=val_labels.tolist(),\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d38b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=32,  \n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"value\"],  \n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b4651de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): XLMRobertaForSequenceClassification(\n",
      "      (roberta): XLMRobertaModel(\n",
      "        (embeddings): XLMRobertaEmbeddings(\n",
      "          (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "          (token_type_embeddings): Embedding(1, 1024)\n",
      "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): XLMRobertaEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-23): 24 x XLMRobertaLayer(\n",
      "              (attention): XLMRobertaAttention(\n",
      "                (self): XLMRobertaSdpaSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=1024, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): XLMRobertaSelfOutput(\n",
      "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): XLMRobertaIntermediate(\n",
      "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): XLMRobertaOutput(\n",
      "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): XLMRobertaClassificationHead(\n",
      "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
      "        )\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): XLMRobertaClassificationHead(\n",
      "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name_roberta, num_labels=2)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "# model = get_peft_model(model, lora_config)\n",
    "model = PeftModel.from_pretrained(model, \"./lora_results_2/checkpoint-76806\")\n",
    "model.config.label2id = {0: 0, 1: 1}\n",
    "model.config.id2label = {0: \"positive\", 1: \"negative\"}\n",
    "model.config.label_names = [\"labels\"]\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3da34a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 1,051,650\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "478cb694",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c1a7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_results_2\",\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    logging_dir=\"./lora_logs_2\",\n",
    "    logging_steps=5000,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,         \n",
    "    metric_for_best_model=\"eval_loss\",    \n",
    "    greater_is_better=False,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,  \n",
    ")\n",
    "\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    class_weights=class_weights,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abb8180f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76806' max='85340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76806/85340 28:06 < 03:07, 45.53 it/s, Epoch 18/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.847240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.778900</td>\n",
       "      <td>0.620821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.619700</td>\n",
       "      <td>0.585643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.506900</td>\n",
       "      <td>0.655266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.481800</td>\n",
       "      <td>0.688369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.413900</td>\n",
       "      <td>0.604725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.413900</td>\n",
       "      <td>0.586632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.397700</td>\n",
       "      <td>0.537869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.359300</td>\n",
       "      <td>0.609466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.333200</td>\n",
       "      <td>0.562897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.284500</td>\n",
       "      <td>0.641368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.264500</td>\n",
       "      <td>0.584677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.231400</td>\n",
       "      <td>0.640496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.231400</td>\n",
       "      <td>0.584224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.228100</td>\n",
       "      <td>0.669112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.169200</td>\n",
       "      <td>0.693264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.178500</td>\n",
       "      <td>0.685739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.752658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=76806, training_loss=0.355220379166481, metrics={'train_runtime': 1687.0571, 'train_samples_per_second': 101.17, 'train_steps_per_second': 50.585, 'total_flos': 3.628414168521523e+16, 'train_loss': 0.355220379166481, 'epoch': 18.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a4c6dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_accuracy(model, dataset, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    val_dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    report = classification_report(all_labels, all_predictions, target_names=[\"positive\", \"negative\"])\n",
    "    print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ae5ba3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.9303\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.96      0.97      0.96      1379\n",
      "    negative       0.61      0.52      0.56       128\n",
      "\n",
      "    accuracy                           0.93      1507\n",
      "   macro avg       0.78      0.74      0.76      1507\n",
      "weighted avg       0.93      0.93      0.93      1507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "measure_accuracy(model, val_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f263e522",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_allegro = \"allegro/herbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_allegro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dfe109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): BertForSequenceClassification(\n",
      "      (bert): BertModel(\n",
      "        (embeddings): BertEmbeddings(\n",
      "          (word_embeddings): Embedding(50000, 768, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 768)\n",
      "          (token_type_embeddings): Embedding(2, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): BertEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x BertLayer(\n",
      "              (attention): BertAttention(\n",
      "                (self): BertSdpaSelfAttention(\n",
      "                  (query): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=32, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=32, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): BertSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): BertIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): BertOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooler): BertPooler(\n",
      "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (activation): Tanh()\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name_allegro, num_labels=2)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "# model = get_peft_model(model, lora_config)\n",
    "model = PeftModel.from_pretrained(model, \"./lora_results_allegro/checkpoint-76806\")\n",
    "model.config.label2id = {0: 0, 1: 1}\n",
    "model.config.id2label = {0: \"positive\", 1: \"negative\"}\n",
    "model.config.label_names = [\"labels\"]\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "392c590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_tokenizer(tokenizer)\n",
    "val_dataset.set_tokenizer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c575c80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_results_allegro\",\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    logging_dir=\"./lora_logs_allegro\",\n",
    "    logging_steps=5000,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,         \n",
    "    metric_for_best_model=\"eval_loss\",    \n",
    "    greater_is_better=False,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,  \n",
    ")\n",
    "\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    class_weights=class_weights,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=7)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c2ab1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64005' max='85340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64005/85340 11:50 < 03:56, 90.12 it/s, Epoch 15/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.599936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.408200</td>\n",
       "      <td>0.586091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.379400</td>\n",
       "      <td>0.602347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.330500</td>\n",
       "      <td>0.605602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.344200</td>\n",
       "      <td>0.598935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.315300</td>\n",
       "      <td>0.605399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.315300</td>\n",
       "      <td>0.574055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.277300</td>\n",
       "      <td>0.544470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.245700</td>\n",
       "      <td>0.660050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.244100</td>\n",
       "      <td>0.625333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.209400</td>\n",
       "      <td>0.666847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.187600</td>\n",
       "      <td>0.601659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.179300</td>\n",
       "      <td>0.672239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.179300</td>\n",
       "      <td>0.656498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.164500</td>\n",
       "      <td>0.700624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=64005, training_loss=0.2660412682526634, metrics={'train_runtime': 710.1847, 'train_samples_per_second': 240.332, 'train_steps_per_second': 120.166, 'total_flos': 8536335929210880.0, 'train_loss': 0.2660412682526634, 'epoch': 15.0})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b499522a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.9277\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.96      0.96      0.96      1379\n",
      "    negative       0.58      0.55      0.56       128\n",
      "\n",
      "    accuracy                           0.93      1507\n",
      "   macro avg       0.77      0.75      0.76      1507\n",
      "weighted avg       0.93      0.93      0.93      1507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "measure_accuracy(model, val_dataset, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
